---
layout:     post
title:      æ·±åº¦å¹³å‡åœºä¸ç»Ÿè®¡ç¥ç»åŠ¨åŠ›å­¦
subtitle:   æ·±åº¦å­¦ä¹ çš„ç”Ÿç‰©ç‰©ç†åŒ–å­¦åŸç† ç¬¬4ç« 
date:       2019-04-15
author:     TablewareBox
header-img: img/post-bg-ios9-web.jpg
catalog: true
tags:
    - DL & BioPhysChem
    - æ·±åº¦å­¦ä¹ ç†è®º
    - å¹³å‡åœºç†è®º
    - ç”Ÿç‰©ç‰©ç†åŒ–å­¦
    - ç»Ÿè®¡åŠ›å­¦
    - æ— åºç³»ç»Ÿ
---

## [æ·±åº¦å­¦ä¹ çš„ç”Ÿç‰©ç‰©ç†åŒ–å­¦åŸç† - Notes Project Overview](https://tablewarebox.github.io/2019/02/16/DL_BioPhysChem_content/)

![knowledge atlas](https://tablewarebox.files.wordpress.com/2018/11/concept-map-81.png)

## å¼•è¨€

**æ·±åº¦å¹³å‡åœºç†è®º**æ˜¯è¿‘å¹´æ¥ Google Brain ç ”ç©¶äººå‘˜æå‡ºçš„ï¼Œç”¨äºè§£é‡Šæ·±åº¦ç¥ç»ç½‘ç»œçš„**è¡¨è¾¾èƒ½åŠ›ã€è®­ç»ƒæŠ€å·§**å’Œ**æ¨¡å‹æ¶æ„**çš„ç†è®ºæ¡†æ¶ï¼Œå…¶æ ¹æºå¯è¿½æº¯åˆ°æ—¥æœ¬å­¦è€…**ç”˜åˆ©ä¿Šä¸€(Shun-ichi Amari)** ä¸Šä¸–çºª70å¹´ä»£æå‡ºçš„**ç»Ÿè®¡ç¥ç»åŠ¨åŠ›å­¦**[^1]ã€‚åœ¨80å¹´ä»£ **Hopfield ç½‘ç»œ**æå‡ºä¹‹åï¼ŒH. Sompolinsky, A. Crisanti å’Œ H. J. Sommers ç­‰äººå°†å…¶å‘å±•ä¸º**è‡ªæ—‹ç»ç’ƒæ¨¡å‹**æ‰€è¡ç”Ÿ**æ— å‘ç¥ç»ç½‘ç»œ**çš„**åŠ¨æ€å¹³å‡åœºç†è®º**[^2]ï¼Œç ”ç©¶ç½‘ç»œè¿è¡Œæ—¶ä¸€èˆ¬æ€§çš„åŠ¨åŠ›å­¦æ€§è´¨ï¼Œå¹¶æè¿°äº†**æœ‰åº-æ··æ²Œç›¸å˜**ã€‚S. S. Schoenholz, S. Ganguli, J. Pennington å’Œ J. Sohl-Dickstein ç­‰äººé‡æ–°ä»¥ç°ä»£çš„**æœ‰å‘ï¼ˆå‰é¦ˆï¼‰ç¥ç»ç½‘ç»œ**ç»“æ„è¿›è¡Œäº†æ¨å¯¼ï¼Œä»–ä»¬çš„ä¸€ç³»åˆ—ç†è®ºå·¥ä½œå·²èƒ½ä¸€å®šç¨‹åº¦ä¸Šä¸ºç½‘ç»œç»“æ„å’Œè®­ç»ƒæŠ€å·§çš„è®¾è®¡æä¾›æŒ‡å¯¼ã€‚

ç¬”è®°çš„è¿™ä¸€ç« èŠ‚æ¶æ„å¦‚ä¸‹ï¼š

- [x] åŠ¨æ€å¹³å‡åœºç†è®ºå›é¡¾[^2]
- [x] æ·±åº¦å¹³å‡åœºï¼šç†è®ºå‡è®¾ä¸é«˜æ–¯è¿‡ç¨‹è§†è§’[^3]
- [ ] æ·±åº¦ç½‘ç»œçš„æŒ‡æ•°çº§è¡¨è¾¾èƒ½åŠ›[^4]
- [ ] æ·±åº¦ä¿¡æ¯ä¼ é€’ï¼šè®­ç»ƒä¸­çš„æœ‰åº-æ··æ²Œç›¸å˜[^5]
- [ ] è®­ç»ƒæŠ€å·§ I: æ®‹å·®ç½‘ç»œçš„è¿è¡ŒåŸç†[^6]
- [ ] è®­ç»ƒæŠ€å·§ II: å±‚å®½åº¦å˜åŒ–ã€å±‚æ–¹å·®å˜åŒ–çš„è¿è¡ŒåŸç†[^7]
- [ ] è®­ç»ƒæŠ€å·§ III: æ‰¹å½’ä¸€åŒ–çš„è¿è¡ŒåŸç†[^8]
- [ ] CNN çš„å¹³å‡åœºç†è®º[^9]
- [ ] RNN, LSTM, GRU çš„å¹³å‡åœºç†è®º[^10][^11]
- [ ] å›¾ç½‘ç»œçš„å¹³å‡åœºç†è®º[^12]

## 4.1 åŠ¨æ€å¹³å‡åœºç†è®ºå›é¡¾

### 4.1.1 æ¨¡å‹å»ºç«‹

ç”˜åˆ©ä¿Šä¸€(Shun-ichi Amari) å»ºç«‹çš„æ¨¡å‹ä¸­ï¼Œ$N$ ä¸ª**ç¥ç»å…ƒ**ç”±è¿ç»­å˜é‡ $\{s_i(t)\in[-1,1]\},\,\,i=1,...,N$ æè¿°ï¼ˆå¯¹åº”è‡ªæ—‹ï¼‰ï¼Œâ€œ**çªè§¦çŸ©é˜µ**â€ $\mathbf{J}$ ï¼ˆå¯¹åº”è‡ªæ—‹çš„è€¦åˆå¸¸æ•°ï¼‰è¡¨è¾¾å®ƒä»¬çš„ç›¸äº’ä½œç”¨ã€‚æ¯ä¸€æ—¶åˆ» $t$ ç¥ç»å…ƒçš„çŠ¶æ€ $s_i(t)$ ç”±**åœº $h_i(t)$** å†³å®šï¼š

$$s_{i}(t)=\phi\left(g h_{i}(t)\right)$$

å…¶ä¸­ $\phi(x)$ ä¸º**éçº¿æ€§æ¿€æ´»å‡½æ•°**ï¼Œå¯é€‰ä¸ºä»»æ„ Så‹å‡½æ•°ï¼Œå¦‚ $\phi(x)=\tanh (x)$ï¼Œéœ€æ»¡è¶³ $\phi(\pm\infty)=\pm 1, \phi(-x)=-\phi(x), \phi'(x)=0.$ 

$g$ ä¸º**éçº¿æ€§æŒ‡æ•°**ï¼š$g\sim 0$ æ—¶ $\phi(x)\sim x$ï¼Œ$g\to \infty$ æ—¶ $\phi(x)\to \pm 1.$

æ—¶é—´æ¼”åŒ–çš„**åŠ¨åŠ›å­¦æ–¹ç¨‹**ä¸º

$$
\partial_{t} h_{i}=-h_{i}+\sum_{j=1}^{N} J_{i j} s_{j}+\theta_i=-h_{i}+\sum_{j=1}^{N} J_{i j} \phi(g h_{j})+\theta_i
$$

è‹¥å°†å…¶ç§»é¡¹ï¼Œå¯ä»¥çœ‹å‡ºå®ƒå’Œå‰é¦ˆç¥ç»ç½‘ç»œçš„å…³ç³»ï¼š

$$
\begin{cases}
    h_{i}+\partial_{t} h_{i}=\sum_{j=1}^{N} J_{i j} s_{j}+\theta_i \\
    s_i=\phi(g h_{i})
\end{cases}
\longleftrightarrow
\begin{cases}
    \mathbf{h}^{l+1}=\mathbf{W}^{l+1} \mathbf{x}^{l}+\mathbf{b}^{l+1} \\
    \mathbf{x}^{l}=\phi\left(\mathbf{h}^{l}\right)
\end{cases}
$$

å®é™…ä¸Šè¿™ä¹Ÿæ˜¯**æ— å‘ç¥ç»ç½‘ç»œ**ä¸**æœ‰å‘ç¥ç»ç½‘ç»œ**çš„å·®åˆ«ï¼šæ— å‘ç¥ç»ç½‘ç»œçš„**æ—¶é—´æ¼”åŒ–**ç±»ä¼¼äºæœ‰å‘ç¥ç»ç½‘ç»œçš„**å±‚é—´ä¼ é€’**ã€‚ä»è¿™ä¸€ç‚¹çœ‹è‡ªæ—‹ç»ç’ƒç±»**æ— å‘ç¥ç»ç½‘ç»œ**å¯ä»¥ç†è§£æˆ**æƒé‡ç›¸åŒçš„æ— é™å±‚æœ‰å‘ç¥ç»ç½‘ç»œ**ã€‚

### 4.1.2 ç†è®ºæ¨å¯¼

ä¸ºå¤„ç†åŠ¨æ€é—®é¢˜ï¼Œå®šä¹‰**æ—¶é—´å…³è”å‡½æ•°**å’Œ**å“åº”å‡½æ•°**çš„**ç”Ÿæˆæ³›å‡½**ï¼š

$$
Z=\int D h D \hat{h} \exp \left\{\sum_{i, t}\left(l_{i}(t) h_{i}(t)+i \hat{l}_{i}(t) i \hat{h}_{i}(t)\right)+L[h, \hat{h}]\right\}
$$

## 4.2 æ·±åº¦å¹³å‡åœºï¼šç†è®ºå‡è®¾ä¸é«˜æ–¯è¿‡ç¨‹è§†è§’

å›åˆ°å¯¹**å‰é¦ˆç¥ç»ç½‘ç»œ**çš„è®¨è®ºã€‚

* ç½‘ç»œå…±æœ‰ $D+1$ å±‚**ç¥ç»å…ƒ** $\mathbf{x}^0,...,\mathbf{x}^D$ï¼Œç¬¬ $l$ å±‚çš„**å®½åº¦**ä¸º $N_l$ï¼Œ
* $D$ å±‚**æƒé‡** $\mathbf{W}^1,...,\mathbf{W}^D$ å’Œ**åç½®** $\mathbf{b}^1,...,\mathbf{b}^D$ã€‚$\mathbf{x}^l, \mathbf{b}^l \in\mathbb{R}^{N_l},\mathbf{W}^l \in\mathbb{R}^{N_l\times N_{l-1}}.$
* å¯¹äº**éšæœºåˆå§‹åŒ–**çš„ç¥ç»ç½‘ç»œï¼Œ $\mathbf{W}_{ij}^l$ å’Œ $\mathbf{b}_{i}^l$ ä¸ºç‹¬ç«‹çš„é›¶å‡å€¼é«˜æ–¯éšæœºå˜é‡ï¼š$\mathbf{W}_{ij}^l \sim \mathcal{N}(0,\sigma_{w}^{2} / N_{l-1}),\,\mathbf{b}_{i}^l \sim \mathcal{N}(0,\sigma_{b}^{2}).$ æ–¹å·®ä½¿å¾— $l-1$ å±‚ç¥ç»å…ƒå¯¹ $l$ å±‚ç¥ç»å…ƒåœºçš„è´¡çŒ®ä¸º $\mathcal{O}(1)$
* å‰å‘ä¼ æ’­çš„åŠ¨åŠ›å­¦ä¸º

$$ 
\mathbf{h}^{l}=\mathbf{W}^{l} \mathbf{x}^{l-1}+\mathbf{b}^{l},\,\,\,\,\,
\mathbf{x}^{l}=\phi(\mathbf{h}^{l})
 $$

## å‚è€ƒæ–‡çŒ®

[^1]: S. I. Amari. **Characteristics of Random Nets of Analog Neuron-Like Elements.** *IEEE Trans. Syst. Man Cybern.* **2**, 643 (1972).

[^2]: H. Sompolinsky, A. Crisanti, and H. J. Sommers. **Chaos in random neural networks.** *Physical Review Letters*, 61(3): 259, 1988.

[^3]: Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S. Schoenholz, Jeffrey Pennington, and Jascha Sohl-Dickstein. **Deep neural networks as Gaussian processes.** *International Conference on Learning Representations*, 2018.

[^4]: Ben Poole, Subhaneil Lahiri, Maithreyi Raghu, Jascha Sohl-Dickstein, and Surya Ganguli. **Exponential expressivity in deep neural networks through transient chaos.** In *Advances In Neural Information Processing Systems*, pages 3360â€“3368, 2016.

[^5]: Samuel S. Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. **Deep Information Propagation.** *International Conference on Learning Representations*, 2017.

[^6]: Greg Yang, and Samuel S. Schoenholz. **Mean field residual networks: On the edge of chaos.** In *Advances in Neural Information Processing Systems*, 2017.

[^7]: Greg Yang and Samuel S. Schoenholz. **Deep mean field theory: Layerwise variance and width variation as methods to control gradient explosion.** *International Conference on Learning Representations*, 2018.

[^8]: Greg Yang, Jeffrey Pennington, Vinay Rao, Jascha Sohl-Dickstein, and Samuel S. Schoenholz. **A mean field theory of batch normalization.** *International Conference on Learning Representations*, 2019.

[^9]: Lechao Xiao, Yasaman Bahri, Jascha Sohl-Dickstein, Samuel S. Schoenholz, and Jeffrey Pennington. **Dynamical isometry and a mean field theory of CNNs: How to train 10,000-layer vanilla convolutional neural networks.** *International Conference on Learning Representations*, 2018.

[^10]: Minmin Chen, Jeffrey Pennington, and Samuel S. Schoenholz. **Dynamical isometry and a mean field theory of RNNs: Gating enables signal propagation in recurrent neural networks.** *International Conference on Learning Representations*, 2018.

[^11]: Dar Gilboa, Bo Chang, Minmin Chen, Greg Yang, Samuel S. Schoenholz, Ed H. Chi, and Jeffrey Pennington. **Dynamical isometry and a mean field theory of LSTMs and GRUs.** *arXiv preprint arXiv:1901.08987*, 2019.

[^12]: Tatsuro Kawamoto, and Masashi Tsubaki. **Mean-field theory of graph neural networks in graph partitioning.** *arXiv preprint arXiv:1810.11908*, 2018.